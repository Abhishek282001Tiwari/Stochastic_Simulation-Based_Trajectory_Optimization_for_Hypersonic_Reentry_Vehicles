{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Uncertainty Analysis\n",
    "\n",
    "This notebook demonstrates comprehensive Monte Carlo uncertainty quantification for hypersonic reentry trajectories.\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "- Understand uncertainty sources in hypersonic reentry\n",
    "- Define uncertain parameters with statistical distributions\n",
    "- Run Monte Carlo simulations with Latin Hypercube Sampling\n",
    "- Analyze uncertainty propagation and statistical results\n",
    "- Create uncertainty visualizations and confidence intervals\n",
    "\n",
    "## ðŸ”¬ Uncertainty Sources\n",
    "\n",
    "Key uncertainty sources in hypersonic reentry include:\n",
    "- **Vehicle parameters**: Mass, drag/lift coefficients, geometry\n",
    "- **Atmospheric conditions**: Density variations, temperature, winds\n",
    "- **Initial conditions**: State estimation errors\n",
    "- **Model uncertainty**: Aerodynamic and thermodynamic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add framework to path\n",
    "sys.path.insert(0, str(Path('..') / 'src'))\n",
    "\n",
    "# Import framework components\n",
    "from hypersonic_reentry.dynamics import VehicleDynamics, VehicleState\n",
    "from hypersonic_reentry.atmosphere import USStandard1976\n",
    "from hypersonic_reentry.uncertainty import UncertaintyQuantifier, UncertainParameter\n",
    "from hypersonic_reentry.utils.constants import DEG_TO_RAD, RAD_TO_DEG\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Matplotlib available: {plt.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define Baseline Configuration\n",
    "\n",
    "Establish the nominal vehicle and atmospheric configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define baseline vehicle parameters\n",
    "vehicle_params = {\n",
    "    'mass': 5000.0,              # kg\n",
    "    'reference_area': 15.0,      # mÂ²\n",
    "    'drag_coefficient': 1.2,     # dimensionless\n",
    "    'lift_coefficient': 0.8,     # dimensionless\n",
    "    'ballistic_coefficient': 400.0,  # kg/mÂ²\n",
    "    'nose_radius': 0.5,          # m\n",
    "    'length': 10.0,              # m\n",
    "    'diameter': 2.0              # m\n",
    "}\n",
    "\n",
    "# Create atmosphere and dynamics\n",
    "atmosphere = USStandard1976()\n",
    "dynamics = VehicleDynamics(vehicle_params, atmosphere_model=atmosphere)\n",
    "\n",
    "# Define baseline initial conditions\n",
    "initial_state = VehicleState(\n",
    "    altitude=120000.0,                    # 120 km\n",
    "    latitude=28.5 * DEG_TO_RAD,          # KSC latitude\n",
    "    longitude=-80.6 * DEG_TO_RAD,        # KSC longitude\n",
    "    velocity=7800.0,                     # m/s\n",
    "    flight_path_angle=-2.0 * DEG_TO_RAD, # degrees\n",
    "    azimuth=90.0 * DEG_TO_RAD,          # eastward\n",
    "    time=0.0\n",
    ")\n",
    "\n",
    "print(\"Baseline Configuration:\")\n",
    "print(f\"Vehicle mass: {vehicle_params['mass']} kg\")\n",
    "print(f\"Entry altitude: {initial_state.altitude/1000:.1f} km\")\n",
    "print(f\"Entry velocity: {initial_state.velocity/1000:.1f} km/s\")\n",
    "print(f\"Entry angle: {initial_state.flight_path_angle*RAD_TO_DEG:.1f}Â°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Uncertain Parameters\n",
    "\n",
    "Specify uncertain parameters with their probability distributions based on engineering judgment and available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define uncertain parameters with realistic uncertainty levels\n",
    "uncertain_params = [\n",
    "    # Vehicle mass uncertainty (Â±5% - fuel consumption, payload variation)\n",
    "    UncertainParameter(\n",
    "        name=\"mass\",\n",
    "        distribution_type=\"normal\",\n",
    "        parameters={\"mean\": 5000.0, \"std\": 250.0},\n",
    "        description=\"Vehicle mass with fuel/payload uncertainty\"\n",
    "    ),\n",
    "    \n",
    "    # Drag coefficient uncertainty (Â±10% - aerodynamic model uncertainty)\n",
    "    UncertainParameter(\n",
    "        name=\"drag_coefficient\",\n",
    "        distribution_type=\"normal\",\n",
    "        parameters={\"mean\": 1.2, \"std\": 0.12},\n",
    "        description=\"Drag coefficient with aerodynamic uncertainty\"\n",
    "    ),\n",
    "    \n",
    "    # Lift coefficient uncertainty (Â±10%)\n",
    "    UncertainParameter(\n",
    "        name=\"lift_coefficient\",\n",
    "        distribution_type=\"normal\", \n",
    "        parameters={\"mean\": 0.8, \"std\": 0.08},\n",
    "        description=\"Lift coefficient with aerodynamic uncertainty\"\n",
    "    ),\n",
    "    \n",
    "    # Reference area uncertainty (Â±5% - manufacturing tolerance)\n",
    "    UncertainParameter(\n",
    "        name=\"reference_area\",\n",
    "        distribution_type=\"normal\",\n",
    "        parameters={\"mean\": 15.0, \"std\": 0.75},\n",
    "        description=\"Reference area with manufacturing uncertainty\"\n",
    "    ),\n",
    "    \n",
    "    # Atmospheric density factor (Â±15% - atmospheric variability)\n",
    "    UncertainParameter(\n",
    "        name=\"atmospheric_density_factor\",\n",
    "        distribution_type=\"log_normal\",\n",
    "        parameters={\"mean\": 1.0, \"std\": 0.15},\n",
    "        description=\"Atmospheric density scaling factor\"\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Uncertain Parameters:\")\n",
    "print(\"=\" * 60)\n",
    "for param in uncertain_params:\n",
    "    print(f\"{param.name:25}: {param.distribution_type:12} - {param.description}\")\n",
    "    if param.distribution_type == \"normal\":\n",
    "        mean = param.parameters[\"mean\"]\n",
    "        std = param.parameters[\"std\"]\n",
    "        cv = (std / mean) * 100\n",
    "        print(f\"{'':27} Î¼={mean:.2f}, Ïƒ={std:.3f} (CV={cv:.1f}%)\")\n",
    "    elif param.distribution_type == \"log_normal\":\n",
    "        mean = param.parameters[\"mean\"]\n",
    "        std = param.parameters[\"std\"]\n",
    "        print(f\"{'':27} Î¼={mean:.2f}, Ïƒ={std:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Visualize Parameter Distributions\n",
    "\n",
    "Plot the probability density functions of the uncertain parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parameter distribution plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, param in enumerate(uncertain_params):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    if param.distribution_type == \"normal\":\n",
    "        mean = param.parameters[\"mean\"]\n",
    "        std = param.parameters[\"std\"]\n",
    "        x = np.linspace(mean - 4*std, mean + 4*std, 1000)\n",
    "        pdf = stats.norm.pdf(x, mean, std)\n",
    "        \n",
    "    elif param.distribution_type == \"log_normal\":\n",
    "        mean = param.parameters[\"mean\"]\n",
    "        std = param.parameters[\"std\"]\n",
    "        # Convert to lognormal parameters\n",
    "        mu = np.log(mean / np.sqrt(1 + (std/mean)**2))\n",
    "        sigma = np.sqrt(np.log(1 + (std/mean)**2))\n",
    "        x = np.linspace(0.01, mean + 4*std, 1000)\n",
    "        pdf = stats.lognorm.pdf(x, sigma, scale=np.exp(mu))\n",
    "    \n",
    "    ax.plot(x, pdf, linewidth=2, label=f'{param.distribution_type.replace(\"_\", \" \").title()}')\n",
    "    ax.fill_between(x, pdf, alpha=0.3)\n",
    "    ax.set_xlabel(param.name.replace('_', ' ').title())\n",
    "    ax.set_ylabel('Probability Density')\n",
    "    ax.set_title(f'{param.name.replace(\"_\", \" \").title()} Distribution')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "# Remove unused subplot\n",
    "if len(uncertain_params) < len(axes):\n",
    "    axes[-1].remove()\n",
    "\n",
    "plt.suptitle('Uncertain Parameter Distributions', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Parameter distributions visualized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set Up Monte Carlo Analysis\n",
    "\n",
    "Configure the uncertainty quantifier with advanced sampling techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create uncertainty quantifier\n",
    "uq = UncertaintyQuantifier(\n",
    "    vehicle_dynamics=dynamics,\n",
    "    uncertain_parameters=uncertain_params,\n",
    "    random_seed=42  # For reproducible results\n",
    ")\n",
    "\n",
    "print(\"ðŸ”¬ Uncertainty Quantifier Configuration:\")\n",
    "print(f\"Number of uncertain parameters: {len(uncertain_params)}\")\n",
    "print(f\"Sampling method: Latin Hypercube Sampling (LHS)\")\n",
    "print(f\"Random seed: 42 (reproducible results)\")\n",
    "print(f\"Base vehicle dynamics: 3-DOF point mass\")\n",
    "\n",
    "# Display sampling information\n",
    "print(\"\\nðŸ“ˆ Sampling Strategy:\")\n",
    "print(\"- Latin Hypercube Sampling for better space-filling\")\n",
    "print(\"- Stratified sampling ensures coverage of parameter space\")\n",
    "print(\"- Each parameter divided into equal-probability intervals\")\n",
    "print(\"- Random permutation prevents parameter correlation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run Monte Carlo Simulation\n",
    "\n",
    "Execute the Monte Carlo analysis with a moderate number of samples for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Monte Carlo analysis\n",
    "print(\"ðŸš€ Starting Monte Carlo simulation...\")\n",
    "print(\"This may take several minutes depending on sample size and complexity.\")\n",
    "\n",
    "# Configure simulation parameters\n",
    "num_samples = 500  # Moderate size for notebook demonstration\n",
    "time_span = (0.0, 1800.0)  # 30 minutes simulation time\n",
    "parallel = False  # Set to True if multiprocessing available\n",
    "\n",
    "print(f\"\\nSimulation Parameters:\")\n",
    "print(f\"Number of samples: {num_samples}\")\n",
    "print(f\"Simulation time: {time_span[1]/60:.1f} minutes\")\n",
    "print(f\"Parallel processing: {parallel}\")\n",
    "\n",
    "# Execute Monte Carlo analysis\n",
    "mc_result = uq.run_monte_carlo_analysis(\n",
    "    initial_state=initial_state,\n",
    "    time_span=time_span,\n",
    "    num_samples=num_samples,\n",
    "    parallel=parallel\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Monte Carlo simulation completed!\")\n",
    "print(f\"Successful simulations: {mc_result.num_samples}\")\n",
    "print(f\"Available outputs: {list(mc_result.mean_values.keys())}\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nðŸ“Š Basic Statistics:\")\n",
    "for output_name, mean_val in mc_result.mean_values.items():\n",
    "    std_val = mc_result.std_values[output_name]\n",
    "    cv = (std_val / abs(mean_val)) * 100 if abs(mean_val) > 1e-10 else 0\n",
    "    print(f\"{output_name:20}: Î¼={mean_val:10.2e}, Ïƒ={std_val:10.2e}, CV={cv:6.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Statistical Analysis of Results\n",
    "\n",
    "Analyze the Monte Carlo results with statistical methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract key performance metrics for detailed analysis\n",
    "outputs_of_interest = ['final_altitude', 'final_velocity', 'downrange', 'flight_time']\n",
    "available_outputs = [output for output in outputs_of_interest if output in mc_result.mean_values]\n",
    "\n",
    "print(\"ðŸ“ˆ DETAILED STATISTICAL ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create DataFrame for easier analysis\n",
    "if hasattr(mc_result, 'raw_data') and 'performance_metrics' in mc_result.raw_data:\n",
    "    df = pd.DataFrame(mc_result.raw_data['performance_metrics'])\n",
    "    \n",
    "    for output in available_outputs:\n",
    "        if output in df.columns:\n",
    "            data = df[output].values\n",
    "            \n",
    "            print(f\"\\n{output.replace('_', ' ').title()}:\")\n",
    "            print(f\"  Mean:       {np.mean(data):12.3e}\")\n",
    "            print(f\"  Std Dev:    {np.std(data):12.3e}\")\n",
    "            print(f\"  Minimum:    {np.min(data):12.3e}\")\n",
    "            print(f\"  Maximum:    {np.max(data):12.3e}\")\n",
    "            print(f\"  Median:     {np.median(data):12.3e}\")\n",
    "            print(f\"  Skewness:   {stats.skew(data):12.3f}\")\n",
    "            print(f\"  Kurtosis:   {stats.kurtosis(data):12.3f}\")\n",
    "            \n",
    "            # Confidence intervals\n",
    "            ci_95 = np.percentile(data, [2.5, 97.5])\n",
    "            ci_90 = np.percentile(data, [5, 95])\n",
    "            \n",
    "            print(f\"  95% CI:     [{ci_95[0]:10.3e}, {ci_95[1]:10.3e}]\")\n",
    "            print(f\"  90% CI:     [{ci_90[0]:10.3e}, {ci_90[1]:10.3e}]\")\n",
    "            \n",
    "else:\n",
    "    print(\"Raw data not available for detailed analysis\")\n",
    "    print(\"Using summary statistics from Monte Carlo result\")\n",
    "    \n",
    "    for output in available_outputs:\n",
    "        if output in mc_result.mean_values:\n",
    "            mean_val = mc_result.mean_values[output]\n",
    "            std_val = mc_result.std_values[output]\n",
    "            \n",
    "            print(f\"\\n{output.replace('_', ' ').title()}:\")\n",
    "            print(f\"  Mean:       {mean_val:12.3e}\")\n",
    "            print(f\"  Std Dev:    {std_val:12.3e}\")\n",
    "            print(f\"  CV:         {(std_val/abs(mean_val)*100):12.1f}%\")\n",
    "            \n",
    "            # Approximate confidence intervals (assuming normal distribution)\n",
    "            ci_95 = [mean_val - 1.96*std_val, mean_val + 1.96*std_val]\n",
    "            ci_90 = [mean_val - 1.645*std_val, mean_val + 1.645*std_val]\n",
    "            \n",
    "            print(f\"  95% CI:     [{ci_95[0]:10.3e}, {ci_95[1]:10.3e}]\")\n",
    "            print(f\"  90% CI:     [{ci_90[0]:10.3e}, {ci_90[1]:10.3e}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create Uncertainty Visualizations\n",
    "\n",
    "Generate comprehensive plots to understand uncertainty propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive uncertainty visualization\n",
    "if hasattr(mc_result, 'raw_data') and 'performance_metrics' in mc_result.raw_data:\n",
    "    df = pd.DataFrame(mc_result.raw_data['performance_metrics'])\n",
    "    \n",
    "    # Set up the plotting layout\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Define outputs to plot\n",
    "    plot_outputs = [col for col in df.columns if col in available_outputs][:4]\n",
    "    \n",
    "    if len(plot_outputs) >= 2:\n",
    "        # 1. Histogram plots\n",
    "        for i, output in enumerate(plot_outputs[:4]):\n",
    "            plt.subplot(3, 2, i+1)\n",
    "            \n",
    "            data = df[output].values\n",
    "            \n",
    "            # Plot histogram with KDE\n",
    "            plt.hist(data, bins=30, alpha=0.7, density=True, color=f'C{i}', edgecolor='black')\n",
    "            \n",
    "            # Add KDE curve\n",
    "            try:\n",
    "                kde = stats.gaussian_kde(data)\n",
    "                x_kde = np.linspace(np.min(data), np.max(data), 200)\n",
    "                plt.plot(x_kde, kde(x_kde), 'r-', linewidth=2, label='KDE')\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Add statistical information\n",
    "            mean_val = np.mean(data)\n",
    "            std_val = np.std(data)\n",
    "            plt.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2e}')\n",
    "            plt.axvline(mean_val-std_val, color='orange', linestyle=':', alpha=0.7)\n",
    "            plt.axvline(mean_val+std_val, color='orange', linestyle=':', alpha=0.7, label='Â±1Ïƒ')\n",
    "            \n",
    "            plt.xlabel(output.replace('_', ' ').title())\n",
    "            plt.ylabel('Probability Density')\n",
    "            plt.title(f'{output.replace(\"_\", \" \").title()} Distribution')\n",
    "            plt.legend(fontsize=8)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 5. Correlation matrix\n",
    "        if len(plot_outputs) >= 2:\n",
    "            plt.subplot(3, 2, 5)\n",
    "            correlation_matrix = df[plot_outputs].corr()\n",
    "            im = plt.imshow(correlation_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "            plt.colorbar(im)\n",
    "            plt.xticks(range(len(plot_outputs)), [col.replace('_', '\\n') for col in plot_outputs], rotation=45)\n",
    "            plt.yticks(range(len(plot_outputs)), [col.replace('_', '\\n') for col in plot_outputs])\n",
    "            plt.title('Output Correlation Matrix')\n",
    "            \n",
    "            # Add correlation values\n",
    "            for i in range(len(plot_outputs)):\n",
    "                for j in range(len(plot_outputs)):\n",
    "                    plt.text(j, i, f'{correlation_matrix.iloc[i,j]:.2f}', \n",
    "                            ha='center', va='center', fontsize=8,\n",
    "                            color='white' if abs(correlation_matrix.iloc[i,j]) > 0.5 else 'black')\n",
    "        \n",
    "        # 6. Scatter plot of two key outputs\n",
    "        if len(plot_outputs) >= 2:\n",
    "            plt.subplot(3, 2, 6)\n",
    "            x_data = df[plot_outputs[0]].values\n",
    "            y_data = df[plot_outputs[1]].values\n",
    "            \n",
    "            plt.scatter(x_data, y_data, alpha=0.6, s=20)\n",
    "            plt.xlabel(plot_outputs[0].replace('_', ' ').title())\n",
    "            plt.ylabel(plot_outputs[1].replace('_', ' ').title())\n",
    "            plt.title(f'{plot_outputs[0]} vs {plot_outputs[1]}'.replace('_', ' ').title())\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add correlation coefficient\n",
    "            corr_coeff = np.corrcoef(x_data, y_data)[0, 1]\n",
    "            plt.text(0.05, 0.95, f'Correlation: {corr_coeff:.3f}', \n",
    "                    transform=plt.gca().transAxes, bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "    \n",
    "    plt.suptitle('Monte Carlo Uncertainty Analysis Results', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ Detailed raw data not available for comprehensive visualization\")\n",
    "    print(\"Creating summary plots based on available statistics...\")\n",
    "    \n",
    "    # Create simple bar plot of coefficients of variation\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Plot means\n",
    "    outputs = list(mc_result.mean_values.keys())\n",
    "    means = list(mc_result.mean_values.values())\n",
    "    \n",
    "    ax1.bar(range(len(outputs)), means)\n",
    "    ax1.set_xticks(range(len(outputs)))\n",
    "    ax1.set_xticklabels([o.replace('_', '\\n') for o in outputs], rotation=45)\n",
    "    ax1.set_ylabel('Mean Value')\n",
    "    ax1.set_title('Mean Performance Metrics')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot coefficients of variation\n",
    "    cvs = [(mc_result.std_values[o]/abs(mc_result.mean_values[o]))*100 \n",
    "           for o in outputs if abs(mc_result.mean_values[o]) > 1e-10]\n",
    "    \n",
    "    ax2.bar(range(len(cvs)), cvs, color='orange')\n",
    "    ax2.set_xticks(range(len(cvs)))\n",
    "    ax2.set_xticklabels([o.replace('_', '\\n') for o in outputs if abs(mc_result.mean_values[o]) > 1e-10], rotation=45)\n",
    "    ax2.set_ylabel('Coefficient of Variation (%)')\n",
    "    ax2.set_title('Relative Uncertainty (CV)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Monte Carlo Analysis Summary', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Uncertainty visualizations created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Sensitivity Analysis Preview\n",
    "\n",
    "Perform a basic correlation-based sensitivity analysis to understand parameter importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform correlation-based sensitivity analysis\n",
    "print(\"ðŸ” PARAMETER SENSITIVITY ANALYSIS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "if hasattr(mc_result, 'raw_data') and 'input_parameters' in mc_result.raw_data:\n",
    "    # Create combined DataFrame with inputs and outputs\n",
    "    input_df = pd.DataFrame(mc_result.raw_data['input_parameters'])\n",
    "    output_df = pd.DataFrame(mc_result.raw_data['performance_metrics'])\n",
    "    \n",
    "    print(\"\\nCorrelation-based Parameter Importance:\")\n",
    "    print(\"(Correlation between input parameters and key outputs)\")\n",
    "    print()\n",
    "    \n",
    "    # Calculate correlations for each output\n",
    "    for output in available_outputs[:3]:  # Limit to first 3 outputs\n",
    "        if output in output_df.columns:\n",
    "            print(f\"\\n{output.replace('_', ' ').title()}:\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            correlations = []\n",
    "            for param in uncertain_params:\n",
    "                param_name = param.name\n",
    "                if param_name in input_df.columns:\n",
    "                    corr = np.corrcoef(input_df[param_name], output_df[output])[0, 1]\n",
    "                    correlations.append((param_name, abs(corr), corr))\n",
    "            \n",
    "            # Sort by absolute correlation\n",
    "            correlations.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            for param_name, abs_corr, corr in correlations:\n",
    "                importance = \"High\" if abs_corr > 0.5 else \"Medium\" if abs_corr > 0.3 else \"Low\"\n",
    "                direction = \"â†‘\" if corr > 0 else \"â†“\" if corr < 0 else \"â—‹\"\n",
    "                print(f\"{param_name:25}: {corr:6.3f} ({abs_corr:5.3f}) {direction} [{importance}]\")\n",
    "    \n",
    "    # Create sensitivity heatmap\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Prepare correlation matrix\n",
    "    param_names = [p.name for p in uncertain_params if p.name in input_df.columns]\n",
    "    output_names = [o for o in available_outputs if o in output_df.columns]\n",
    "    \n",
    "    corr_matrix = np.zeros((len(output_names), len(param_names)))\n",
    "    \n",
    "    for i, output in enumerate(output_names):\n",
    "        for j, param in enumerate(param_names):\n",
    "            corr_matrix[i, j] = np.corrcoef(input_df[param], output_df[output])[0, 1]\n",
    "    \n",
    "    # Plot heatmap\n",
    "    im = ax.imshow(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1, aspect='auto')\n",
    "    \n",
    "    # Set ticks and labels\n",
    "    ax.set_xticks(range(len(param_names)))\n",
    "    ax.set_yticks(range(len(output_names)))\n",
    "    ax.set_xticklabels([p.replace('_', '\\n') for p in param_names], rotation=45)\n",
    "    ax.set_yticklabels([o.replace('_', '\\n') for o in output_names])\n",
    "    \n",
    "    # Add correlation values\n",
    "    for i in range(len(output_names)):\n",
    "        for j in range(len(param_names)):\n",
    "            text = ax.text(j, i, f'{corr_matrix[i, j]:.2f}',\n",
    "                          ha=\"center\", va=\"center\", color=\"black\" if abs(corr_matrix[i, j]) < 0.5 else \"white\")\n",
    "    \n",
    "    ax.set_title('Parameter-Output Correlation Matrix')\n",
    "    plt.colorbar(im, ax=ax, label='Correlation Coefficient')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"\\nâš ï¸ Input parameter data not available for sensitivity analysis\")\n",
    "    print(\"This typically requires saving intermediate results during Monte Carlo simulation\")\n",
    "    print(\"\\nFor comprehensive sensitivity analysis, see notebook: 05_sensitivity_analysis.ipynb\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ Sensitivity analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Findings and Insights\n",
    "\n",
    "Based on the Monte Carlo analysis, we can draw several important conclusions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize key findings\n",
    "print(\"ðŸ” KEY FINDINGS FROM MONTE CARLO ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n1. ðŸ“Š STATISTICAL PROPERTIES:\")\n",
    "for output in available_outputs[:3]:\n",
    "    if output in mc_result.mean_values:\n",
    "        mean_val = mc_result.mean_values[output]\n",
    "        std_val = mc_result.std_values[output]\n",
    "        cv = (std_val / abs(mean_val)) * 100 if abs(mean_val) > 1e-10 else 0\n",
    "        \n",
    "        print(f\"   {output.replace('_', ' ').title()}:\")\n",
    "        print(f\"     - Coefficient of Variation: {cv:.1f}%\")\n",
    "        \n",
    "        uncertainty_level = \"High\" if cv > 20 else \"Medium\" if cv > 10 else \"Low\"\n",
    "        print(f\"     - Uncertainty Level: {uncertainty_level}\")\n",
    "        print()\n",
    "\n",
    "print(\"2. ðŸŽ¯ DESIGN IMPLICATIONS:\")\n",
    "print(\"   - Parameters with CV > 15% require careful design margins\")\n",
    "print(\"   - Strong correlations indicate coupled design dependencies\")\n",
    "print(\"   - Non-normal distributions suggest nonlinear system behavior\")\n",
    "print()\n",
    "\n",
    "print(\"3. ðŸ”§ RECOMMENDATIONS:\")\n",
    "print(\"   - Focus uncertainty reduction on most influential parameters\")\n",
    "print(\"   - Design robust control systems for high-uncertainty outputs\")\n",
    "print(\"   - Use confidence intervals for mission planning and safety margins\")\n",
    "print(\"   - Consider Monte Carlo results in optimization formulations\")\n",
    "print()\n",
    "\n",
    "print(\"4. ðŸ“ˆ NEXT STEPS:\")\n",
    "print(\"   - Increase sample size for production analysis (1000+ samples)\")\n",
    "print(\"   - Perform Sobol sensitivity analysis for variance decomposition\")\n",
    "print(\"   - Include more uncertainty sources (atmospheric, model uncertainty)\")\n",
    "print(\"   - Validate results with experimental data when available\")\n",
    "\n",
    "print(f\"\\n\\nðŸŽ‰ Monte Carlo analysis completed with {mc_result.num_samples} samples!\")\n",
    "print(f\"ðŸ“Š Statistical confidence achieved for design decision-making\")\n",
    "print(f\"ðŸš€ Ready for robust trajectory optimization and mission planning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Next Steps\n",
    "\n",
    "Now that you've mastered Monte Carlo uncertainty analysis:\n",
    "\n",
    "1. **Trajectory Optimization**: `04_trajectory_optimization.ipynb`\n",
    "   - Incorporate uncertainty in optimization formulations\n",
    "   - Robust optimization under uncertainty\n",
    "\n",
    "2. **Advanced Sensitivity Analysis**: `05_sensitivity_analysis.ipynb`\n",
    "   - Sobol indices for variance-based sensitivity\n",
    "   - Morris screening for factor prioritization\n",
    "\n",
    "3. **Custom Analysis**: Extend this notebook by:\n",
    "   - Adding more uncertain parameters\n",
    "   - Increasing sample size for production runs\n",
    "   - Including atmospheric uncertainty models\n",
    "   - Implementing advanced sampling techniques\n",
    "\n",
    "## ðŸ“š References\n",
    "\n",
    "- Saltelli, A. et al. \"Global Sensitivity Analysis: The Primer\" (2008)\n",
    "- McKay, M.D. et al. \"A Comparison of Three Methods for Selecting Values of Input Variables\" (1979)\n",
    "- Sobol, I.M. \"Global sensitivity indices for nonlinear mathematical models\" (2001)\n",
    "\n",
    "**Happy uncertainty quantification! ðŸŽ²ðŸ“Š**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}